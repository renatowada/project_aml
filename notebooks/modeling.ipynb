{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e65aa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18792ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../data/01_raw/metrics_df.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5638b031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            in_degree  out_degree  total_in  total_out  avg_retention_hours  \\\n",
      "account_id                                                                    \n",
      "acc599093           8           4  44956.33   33279.32           138.147569   \n",
      "acc194554           6           5  10606.53   25456.37            22.116546   \n",
      "acc729547           3           5   3789.99   33196.35           168.574259   \n",
      "acc933654           9           6  65752.06    6654.72          -313.236806   \n",
      "acc838332           5           2   9436.99    1159.69            89.337556   \n",
      "\n",
      "               ratio  is_suspect  is_fraud    role  \n",
      "account_id                                          \n",
      "acc599093   0.740259           0         0  Honest  \n",
      "acc194554   2.400064           0         0  Honest  \n",
      "acc729547   8.758931           0         0  Honest  \n",
      "acc933654   0.101209           0         0  Honest  \n",
      "acc838332   0.122888           0         0  Honest  \n"
     ]
    }
   ],
   "source": [
    "labels_df = pd.read_csv(\"../data/01_raw/accounts_labels.csv\").set_index('account_id')\n",
    "\n",
    "metrics_df = df.join(labels_df, how='inner')\n",
    "\n",
    "print(metrics_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adf42306",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = metrics_df[['in_degree', 'out_degree', 'total_in', 'ratio', 'avg_retention_hours']]\n",
    "\n",
    "y = metrics_df['is_fraud'] # Em uma situação real essa informação não estaria disponível, mas já que eu tenho..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6ba05c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjunto de treino == 70%\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb2c4988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjunto de Validação == 15%\n",
    "# Conjunto de Teste == 15%\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=12345)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6379c9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9408948b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Métricas de Regressão Logística---\n",
      "AUC-ROC Score: 1.00\n",
      "Recall Score: 1.00\n",
      "Precision Score: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Regressão Logística\n",
    "model_lr = LogisticRegression(max_iter=10000, class_weight='balanced', random_state=12345)\n",
    "model_lr.fit(X_train, y_train)\n",
    "probs_lr = model_lr.predict_proba(X_val)[:, 1] # Para auc-roc\n",
    "preds_lr = model_lr.predict(X_val)             # Para precision e recall\n",
    "auc_lr = roc_auc_score(y_val, probs_lr)\n",
    "recall_lr = recall_score(y_val, preds_lr)\n",
    "precision_lr = precision_score(y_val, preds_lr)\n",
    "\n",
    "print('---Métricas de Regressão Logística---')\n",
    "print(f\"AUC-ROC Score: {auc_lr:.2f}\")\n",
    "print(f\"Recall Score: {recall_lr:.2f}\")\n",
    "print(f\"Precision Score: {precision_lr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6a57925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Métricas de Random Forest---\n",
      "AUC-ROC Score: 1.00\n",
      "Recall Score: 1.00\n",
      "Precision Score: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "model_rf = RandomForestClassifier(class_weight='balanced', random_state=12345)\n",
    "model_rf.fit(X_train, y_train)\n",
    "probs_rf = model_rf.predict_proba(X_val)[:, 1]\n",
    "preds_rf = model_rf.predict(X_val)\n",
    "auc_rf = roc_auc_score(y_val, probs_rf)\n",
    "recall_rf = recall_score(y_val, preds_rf)\n",
    "precision_rf = precision_score(y_val, preds_rf)\n",
    "\n",
    "print('---Métricas de Random Forest---')\n",
    "print(f\"AUC-ROC Score: {auc_rf:.2f}\")\n",
    "print(f\"Recall Score: {recall_rf:.2f}\")\n",
    "print(f\"Precision Score: {precision_rf:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa012f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Métricas de XGBM---\n",
      "AUC-ROC Score: 1.00\n",
      "Recall Score: 0.95\n",
      "Precision Score: 0.93\n"
     ]
    }
   ],
   "source": [
    "# XGBM\n",
    "model_xgb = XGBClassifier(n_estimators=100, random_state=12345, learning_rate=0.05)\n",
    "model_xgb.fit(X_train, y_train)\n",
    "probs_xgb = model_xgb.predict_proba(X_val)[:, 1]\n",
    "preds_xgb = model_xgb.predict(X_val)\n",
    "auc_xgb = roc_auc_score(y_val, probs_xgb)\n",
    "recall_xgb = recall_score(y_val, preds_xgb)\n",
    "precision_xgb = precision_score(y_val, preds_xgb)\n",
    "\n",
    "print('---Métricas de XGBM---')\n",
    "print(f\"AUC-ROC Score: {auc_xgb:.2f}\")\n",
    "print(f\"Recall Score: {recall_xgb:.2f}\")\n",
    "print(f\"Precision Score: {precision_xgb:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e77109a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Métricas de Teste para XGBM---\n",
      "AUC-ROC Score: 1.00\n",
      "Recall Score: 0.96\n",
      "Precision Score: 0.91\n"
     ]
    }
   ],
   "source": [
    "probs_test = model_xgb.predict_proba(X_test)[:, 1]\n",
    "preds_test = model_xgb.predict(X_test)\n",
    "\n",
    "auc_test = roc_auc_score(y_test, probs_test)\n",
    "recall_test = recall_score(y_test, preds_test)\n",
    "precision_test = precision_score(y_test, preds_test)\n",
    "\n",
    "print('---Métricas de Teste para XGBM---')\n",
    "print(f\"AUC-ROC Score: {auc_test:.2f}\")\n",
    "print(f\"Recall Score: {recall_test:.2f}\")\n",
    "print(f\"Precision Score: {precision_test:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
